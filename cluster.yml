---
- hosts: all
  become: yes

  vars:
    - hadoop_version: "3.3.4"
    - hadoop_sha512_checksum: "ca5e12625679ca95b8fd7bb7babc2a8dcb2605979b901df9ad137178718821097b67555115fafc6dbf6bb32b61864ccb6786dbc555e589694a22bf69147780b4"
    - hdfs_root: "/home/{{ ansible_user }}/data"
    - hadoop_home: "/home/{{ ansible_user }}/hadoop"

  vars_files:
    - vars.yml

  pre_tasks:

    - include_tasks: configure_hostnames.yml

    - name: Update apt cache
      apt: update_cache=yes cache_valid_time=3600  # 1 hr

    - name: Check if hdfs is already set up
      stat:
        path: "{{ hdfs_root }}"
      register: hdfs_root_result

    - name: Perform a comprehensive upgrade if hdfs is not set up
      apt: upgrade=dist
      when: hdfs_root_result.stat.isdir is not defined or not hdfs_root_result.stat.isdir
      notify: reboot server

    - name: Perform safe upgrade
      # Safe upgrade only if hdfs is set up to limit any data loss risks
      apt: upgrade=safe
      when: hdfs_root_result.stat.isdir is defined and hdfs_root_result.stat.isdir

  tasks:

    - include_tasks: master_cluster_ssh_access.yml

    - name: Ensure that we have the jdk installed
      apt: pkg=openjdk-11-jdk state=present

    - include_tasks: setup_hadoop_distribution.yml

    - include_tasks: configure_hadoop.yml

  handlers:
    - name: reboot server
      reboot: msg="Rebooting host"

    - name: restart networking
      service: name=networking state=restarted
